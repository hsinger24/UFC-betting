{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras import regularizers\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_classifier(function, param_grid, cv = 4):\n",
    "    # Running Grid Search\n",
    "    grid_search = GridSearchCV(function, param_grid, cv = cv)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # Outputting results\n",
    "    print(f'Best parameters are: {grid_search.best_params_}\\n')\n",
    "    print(f'Accuracy is: {grid_search.score(x_test, y_test)}\\n')\n",
    "    try:\n",
    "        print(f'AUC score is: {roc_auc_score(y_test, grid_search.predict_proba(x_test)[:, 1])}\\n')\n",
    "    except:\n",
    "        pass\n",
    "    best_model = grid_search.best_estimator_\n",
    "    pred_rf = best_model.predict(x_test)\n",
    "    print(f'Classification report:\\n {classification_report(y_test, pred_rf, target_names = [\"Lower Ranked\", \"Higher Ranked\"])}')\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mma_data.csv', index_col=0)\n",
    "\n",
    "# Filtering out unwanted rows\n",
    "data = data[data.result >= 0]\n",
    "data = data[data.slpm_2 + data.sapm_2 != 0]\n",
    "data = data[data.slpm_1 + data.sapm_1 != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering some columns\n",
    "data['reach_diff'] = data.reach_1 - data.reach_2\n",
    "data['age_diff'] = data.age_1 - data.age_2\n",
    "data['slpm_diff'] = data.slpm_1 - data.slpm_2\n",
    "data['sapm_diff'] = data.sapm_1 - data.sapm_2\n",
    "data['td_acc_diff'] = data.td_acc_1 - data.td_acc_2\n",
    "data['td_def_diff'] = data.td_def_1 - data.td_def_2\n",
    "data['td_avg_diff'] = data.td_avg_1 - data.td_avg_2\n",
    "data['sub_avg_diff'] = data.sub_avg_1 - data.sub_avg_2\n",
    "data['strk_acc_diff'] = data.strk_acc_1 - data.strk_acc_2\n",
    "data['strk_def_diff'] = data.strk_def_1 - data.strk_def_2\n",
    "data['wins_diff'] = data.wins_1 - data.wins_2\n",
    "data['losses_diff'] = data.losses_1 - data.losses_2\n",
    "data['win_pct_1'] = data.wins_1/(data.losses_1 + data.wins_1)\n",
    "data['win_pct_2'] = data.wins_2/(data.losses_2 + data.wins_2)\n",
    "data['win_pct_diff'] = data.win_pct_1 - data.win_pct_2\n",
    "\n",
    "# Droping unecessary columnns and scaling data\n",
    "data.drop(['fighter_1', 'fighter_2'], axis = 1, inplace = True)\n",
    "x_cols = ['reach_diff', 'age_diff', 'slpm_diff', 'sapm_diff', 'td_acc_diff', 'td_def_diff',\n",
    "              'td_avg_diff', 'sub_avg_diff', 'strk_acc_diff', 'strk_def_diff', 'wins_diff',\n",
    "              'losses_diff', 'win_pct_diff', 'weight_1']\n",
    "y_col = ['result']\n",
    "x, y = data[x_cols], data[y_col]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Formatting data\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values.ravel()\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting fighter 1 would yield 57.57575757575758% accuracy\n",
      "Predicting the fighter with the higher win percentage would yield 56.060606060606055% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicting fighter 1 would yield {data.result.mean()*100}% accuracy')\n",
    "\n",
    "df = data.copy()\n",
    "df['Higher_Pct'] = df.win_pct_diff.apply(lambda x: 1 if x > 0 else 0)\n",
    "df['Result_Tracker'] = df.apply(lambda x: 1 if (x.Higher_Pct == 1) & (x.result == 1)\n",
    "                               else 1 if (x.Higher_Pct == 0) & (x.result == 0)\n",
    "                               else 0, axis = 1)\n",
    "print(f'Predicting the fighter with the higher win percentage would yield {df.Result_Tracker.mean()*100}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Instantiating NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=x_train_scaled.shape[1],\n",
    "                activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.5487805008888245\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train_scaled, y=y_train, epochs=200, batch_size=64, verbose=0)\n",
    "test_results = model.evaluate(x = x_test_scaled, y = y_test, verbose=0)\n",
    "print(\"Test Accuracy = {}\".format(test_results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 4, 'max_features': 8, 'n_estimators': 15}\n",
      "\n",
      "Accuracy is: 0.6021505376344086\n",
      "\n",
      "AUC score is: 0.6666666666666666\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.72      0.38      0.49        48\n",
      "Higher Ranked       0.56      0.84      0.67        45\n",
      "\n",
      "     accuracy                           0.60        93\n",
      "    macro avg       0.64      0.61      0.58        93\n",
      " weighted avg       0.64      0.60      0.58        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "rf = grid_search_classifier(RandomForestClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 5, 'max_features': 5, 'n_estimators': 5}\n",
      "\n",
      "Accuracy is: 0.5591397849462365\n",
      "\n",
      "AUC score is: 0.5752314814814814\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.77      0.21      0.33        48\n",
      "Higher Ranked       0.53      0.93      0.67        45\n",
      "\n",
      "     accuracy                           0.56        93\n",
      "    macro avg       0.65      0.57      0.50        93\n",
      " weighted avg       0.65      0.56      0.49        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "gb = grid_search_classifier(GradientBoostingClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 100}\n",
      "\n",
      "Accuracy is: 0.5806451612903226\n",
      "\n",
      "AUC score is: 0.648611111111111\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.70      0.33      0.45        48\n",
      "Higher Ranked       0.54      0.84      0.66        45\n",
      "\n",
      "     accuracy                           0.58        93\n",
      "    macro avg       0.62      0.59      0.56        93\n",
      " weighted avg       0.62      0.58      0.55        93\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "lr = grid_search_classifier(LogisticRegression(random_state = 0, max_iter = 500), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 100, 'degree': 2, 'kernel': 'poly'}\n",
      "\n",
      "Accuracy is: 0.6344086021505376\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.89      0.33      0.48        48\n",
      "Higher Ranked       0.57      0.96      0.72        45\n",
      "\n",
      "     accuracy                           0.63        93\n",
      "    macro avg       0.73      0.64      0.60        93\n",
      " weighted avg       0.74      0.63      0.60        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "kernel = ['rbf', 'poly', 'sigmoid']\n",
    "degree = [int(x) for x in np.linspace(start = 2, stop = 7, num = 5)]\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c,\n",
    "    'kernel' : kernel,\n",
    "    'degree' : degree\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "svm = grid_search_classifier(SVC(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "var_smoothing = [int(1**x) for x in np.linspace(start = -9, stop = 0, num = 10)]\n",
    "param_grid = {\n",
    "    'var_smoothing' : var_smoothing\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "grid_search_classifier(GaussianNB(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 0.01}\n",
      "\n",
      "Accuracy is: 0.6103896103896104\n",
      "\n",
      "AUC score is: 0.6336088154269972\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.57      0.36      0.44        33\n",
      "Higher Ranked       0.62      0.80      0.70        44\n",
      "\n",
      "     accuracy                           0.61        77\n",
      "    macro avg       0.60      0.58      0.57        77\n",
      " weighted avg       0.60      0.61      0.59        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = x.values\n",
    "\n",
    "# Iterating over models to create prediction columns\n",
    "models = [rf, gb, lr, svm]\n",
    "names = ['rf', 'gb', 'lr', 'svm']\n",
    "for model, name in zip(models, names):\n",
    "    try:\n",
    "        data[f'{name}_pred'] = model.predict_proba(x)\n",
    "    except:\n",
    "        data[f'{name}_pred'] = model.predict(x)\n",
    "\n",
    "# Re-formatting data\n",
    "x_ensemble, y_ensemble = data[x_cols], data[y_col]\n",
    "x_train_ensemble, x_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(x_ensemble, y_ensemble, test_size = 0.2, random_state = 0)\n",
    "\n",
    "x_train_ensemble = x_train_ensemble.values\n",
    "y_train_ensemble = y_train_ensemble.values.ravel()\n",
    "x_test_ensemble = x_test_ensemble.values\n",
    "y_test_ensemble = y_test_ensemble.values.ravel()\n",
    "\n",
    "# Creating parameter grid\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c\n",
    "}\n",
    "\n",
    "# Running Logistic\n",
    "\n",
    "# Running Grid Search\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state = 0, max_iter = 500), param_grid, cv = 4)\n",
    "grid_search.fit(x_train_ensemble, y_train_ensemble)\n",
    "\n",
    "# Outputting results\n",
    "print(f'Best parameters are: {grid_search.best_params_}\\n')\n",
    "print(f'Accuracy is: {grid_search.score(x_test_ensemble, y_test_ensemble)}\\n')\n",
    "try:\n",
    "    print(f'AUC score is: {roc_auc_score(y_test_ensemble, grid_search.predict_proba(x_test)[:, 1])}\\n')\n",
    "except:\n",
    "    pass\n",
    "ensemble = grid_search.best_estimator_\n",
    "pred_rf = ensemble.predict(x_test_ensemble)\n",
    "print(f'Classification report:\\n {classification_report(y_test_ensemble, pred_rf, target_names = [\"Lower Ranked\", \"Higher Ranked\"])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['KO_OVR']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Formatting data\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values.ravel()\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting no knockout would yield 62.836185819070906% accuracy\n",
      "Predicting the fighter with the higher SLPM would yield 57.21271393643031% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f'Always predicting no knockout would yield {100 - data.KO_OVR.mean()*100}% accuracy')\n",
    "\n",
    "df = data.copy()\n",
    "df['Higher_Pct'] = df.slpm_1.apply(lambda x: 1 if x > 0 else 0)\n",
    "df['Result_Tracker'] = df.apply(lambda x: 1 if (x.Higher_Pct == 1) & (x.result == 1)\n",
    "                               else 1 if (x.Higher_Pct == 0) & (x.result == 0)\n",
    "                               else 0, axis = 1)\n",
    "print(f'Predicting the fighter with the higher SLPM would yield {df.Result_Tracker.mean()*100}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 6, 'max_features': 8, 'n_estimators': 15}\n",
      "\n",
      "Accuracy is: 0.524390243902439\n",
      "\n",
      "AUC score is: 0.4155844155844156\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.58      0.76      0.65        49\n",
      "Higher Ranked       0.33      0.18      0.24        33\n",
      "\n",
      "     accuracy                           0.52        82\n",
      "    macro avg       0.46      0.47      0.45        82\n",
      " weighted avg       0.48      0.52      0.49        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "rf = grid_search_classifier(RandomForestClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 10, 'max_features': 10, 'n_estimators': 6}\n",
      "\n",
      "Accuracy is: 0.5121951219512195\n",
      "\n",
      "AUC score is: 0.4001236858379716\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.57      0.76      0.65        49\n",
      "Higher Ranked       0.29      0.15      0.20        33\n",
      "\n",
      "     accuracy                           0.51        82\n",
      "    macro avg       0.43      0.45      0.42        82\n",
      " weighted avg       0.46      0.51      0.47        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "gb = grid_search_classifier(GradientBoostingClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 0.1}\n",
      "\n",
      "Accuracy is: 0.5487804878048781\n",
      "\n",
      "AUC score is: 0.45701917130488556\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.59      0.80      0.68        49\n",
      "Higher Ranked       0.38      0.18      0.24        33\n",
      "\n",
      "     accuracy                           0.55        82\n",
      "    macro avg       0.48      0.49      0.46        82\n",
      " weighted avg       0.50      0.55      0.50        82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "lr = grid_search_classifier(LogisticRegression(random_state = 0, max_iter = 500), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 10, 'degree': 5, 'kernel': 'poly'}\n",
      "\n",
      "Accuracy is: 0.5609756097560976\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.59      0.84      0.69        49\n",
      "Higher Ranked       0.38      0.15      0.22        33\n",
      "\n",
      "     accuracy                           0.56        82\n",
      "    macro avg       0.49      0.49      0.46        82\n",
      " weighted avg       0.51      0.56      0.50        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "kernel = ['rbf', 'poly', 'sigmoid']\n",
    "degree = [int(x) for x in np.linspace(start = 2, stop = 7, num = 5)]\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c,\n",
    "    'kernel' : kernel,\n",
    "    'degree' : degree\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "svm = grid_search_classifier(SVC(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting SUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsampling data to get a higher % of submissions\n",
    "\n",
    "sub_data = data[data.SUB_OVR == 1]\n",
    "non_sub_data = data[data.SUB_OVR == 0].sample(sub_data.shape[0])\n",
    "data_sub = pd.concat([sub_data, non_sub_data])\n",
    "\n",
    "# Splitting data for training\n",
    "y_col = ['SUB_OVR']\n",
    "x, y = data_sub[x_cols], data_sub[y_col]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Formatting data\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values.ravel()\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting no sub would yield 80.73593073593074% accuracy\n"
     ]
    }
   ],
   "source": [
    "print(f'Always predicting no sub would yield {100 - data.SUB_OVR.mean()*100}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 6, 'max_features': 4, 'n_estimators': 13}\n",
      "\n",
      "Accuracy is: 0.5\n",
      "\n",
      "AUC score is: 0.40625\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.46      0.75      0.57        16\n",
      "Higher Ranked       0.60      0.30      0.40        20\n",
      "\n",
      "     accuracy                           0.50        36\n",
      "    macro avg       0.53      0.53      0.49        36\n",
      " weighted avg       0.54      0.50      0.48        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "rf = grid_search_classifier(RandomForestClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'max_depth': 2, 'max_features': 3, 'n_estimators': 4}\n",
      "\n",
      "Accuracy is: 0.4444444444444444\n",
      "\n",
      "AUC score is: 0.45625\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.42      0.69      0.52        16\n",
      "Higher Ranked       0.50      0.25      0.33        20\n",
      "\n",
      "     accuracy                           0.44        36\n",
      "    macro avg       0.46      0.47      0.43        36\n",
      " weighted avg       0.47      0.44      0.42        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "n_estimators = [int(x) for x in np.linspace(start = 3, stop = 15, num = 13)]\n",
    "max_features = [int(x) for x in np.linspace(start = 3, stop = 10, num = 8)]\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)]\n",
    "param_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_features' : max_features,\n",
    "    'max_depth' : max_depth\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "gb = grid_search_classifier(GradientBoostingClassifier(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 10}\n",
      "\n",
      "Accuracy is: 0.4166666666666667\n",
      "\n",
      "AUC score is: 0.409375\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.38      0.50      0.43        16\n",
      "Higher Ranked       0.47      0.35      0.40        20\n",
      "\n",
      "     accuracy                           0.42        36\n",
      "    macro avg       0.42      0.42      0.42        36\n",
      " weighted avg       0.43      0.42      0.41        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/hsinger24/Desktop/Programming/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "lr = grid_search_classifier(LogisticRegression(random_state = 0, max_iter = 500), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'C': 10, 'degree': 5, 'kernel': 'poly'}\n",
      "\n",
      "Accuracy is: 0.5833333333333334\n",
      "\n",
      "Classification report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      " Lower Ranked       0.54      0.44      0.48        16\n",
      "Higher Ranked       0.61      0.70      0.65        20\n",
      "\n",
      "     accuracy                           0.58        36\n",
      "    macro avg       0.57      0.57      0.57        36\n",
      " weighted avg       0.58      0.58      0.58        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating parameter grid\n",
    "\n",
    "kernel = ['rbf', 'poly', 'sigmoid']\n",
    "degree = [int(x) for x in np.linspace(start = 2, stop = 7, num = 5)]\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {\n",
    "    'C' : c,\n",
    "    'kernel' : kernel,\n",
    "    'degree' : degree\n",
    "}\n",
    "\n",
    "# Running ML function\n",
    "svm = grid_search_classifier(SVC(random_state = 0), param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "data_scaled = pd.DataFrame(StandardScaler().fit_transform(data), columns = data.columns, index = data.index)\n",
    "\n",
    "# Fitting PCA\n",
    "pca = PCA().fit(data_scaled)\n",
    "\n",
    "# Definining and executing function to get weights\n",
    "def PC_Weights():\n",
    "    weights = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(pca.components_)):\n",
    "        weights[\"weights_{}\".format(i)] = pca.components_[i] / sum(pca.components_[i])\n",
    "        \n",
    "    weights = weights.values.T\n",
    "    return weights\n",
    "\n",
    "weights = PC_Weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcfebd83890>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explained variance\n",
    "\n",
    "data_dict = {\n",
    "    'Explained Variance' : list(pca.explained_variance_ratio_[:3]),\n",
    "    'Eigen Number' : list(range(3))\n",
    "}\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.plot.barh(title = \"Explained variance\", x = 'Eigen Number', y = 'Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphing top Eigenvectors\n",
    "Num_Comps = 3\n",
    "top_eigens = pd.DataFrame(weights[:Num_Comps], columns = data_scaled.columns)\n",
    "top_eigens.index = [f'Principal Component {i}' for i in range(1, Num_Comps + 1)]\n",
    "\n",
    "axes = top_eigens.T.plot.bar(subplots = True, legend = False, figsize = (14, 10))\n",
    "plt.subplots_adjust(hspace = 0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "data = pd.read_csv('mma_data_odds.csv', index_col = 0)\n",
    "\n",
    "# Engineering some columns\n",
    "data['reach_diff'] = data.reach_1 - data.reach_2\n",
    "data['age_diff'] = data.age_1 - data.age_2\n",
    "data['slpm_diff'] = data.slpm_1 - data.slpm_2\n",
    "data['sapm_diff'] = data.sapm_1 - data.sapm_2\n",
    "data['td_acc_diff'] = data.td_acc_1 - data.td_acc_2\n",
    "data['td_def_diff'] = data.td_def_1 - data.td_def_2\n",
    "data['td_avg_diff'] = data.td_avg_1 - data.td_avg_2\n",
    "data['sub_avg_diff'] = data.sub_avg_1 - data.sub_avg_2\n",
    "data['strk_acc_diff'] = data.strk_acc_1 - data.strk_acc_2\n",
    "data['strk_def_diff'] = data.strk_def_1 - data.strk_def_2\n",
    "data['wins_diff'] = data.wins_1 - data.wins_2\n",
    "data['losses_diff'] = data.losses_1 - data.losses_2\n",
    "data['win_pct_1'] = data.wins_1/(data.losses_1 + data.wins_1)\n",
    "data['win_pct_2'] = data.wins_2/(data.losses_2 + data.wins_2)\n",
    "data['win_pct_diff'] = data.win_pct_1 - data.win_pct_2\n",
    "\n",
    "# Droping unecessary columnns and scaling data\n",
    "x_cols = ['reach_diff', 'age_diff', 'slpm_diff', 'sapm_diff', 'td_acc_diff', 'td_def_diff',\n",
    "              'td_avg_diff', 'sub_avg_diff', 'strk_acc_diff', 'strk_def_diff', 'wins_diff',\n",
    "              'losses_diff', 'win_pct_diff', 'weight_1']\n",
    "x_predict = data[x_cols]\n",
    "\n",
    "# Formatting data\n",
    "x_predict = x_predict.values\n",
    "\n",
    "# Putting model predictions into the data\n",
    "\n",
    "data['Prediction_1_lr'] = lr.predict_proba(x_predict)[:, 0]\n",
    "data['Prediction_2_lr'] = 1.0 - data.Prediction_1_lr\n",
    "data['Prediction_rf'] = rf.predict(x_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21595.289520879956"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create bet size\n",
    "def rf_bets(row):\n",
    "    if row.Prediction_rf == 1:\n",
    "        if (row.Fighter_1_Odds > 100) & (row.Fighter_1_Odds < 150):\n",
    "            bet = 100\n",
    "        elif (row.Fighter_1_Odds >= 150) & (row.Fighter_1_Odds < 200):\n",
    "            bet = 250\n",
    "        elif (row.Fighter_1_Odds >= 200):\n",
    "            bet = 500\n",
    "        elif (row.Fighter_1_Odds <= 100) & (row.Fighter_1_Odds > -150):\n",
    "            bet = 75\n",
    "        elif (row.Fighter_1_Odds <= -150) & (row.Fighter_1_Odds > -200):\n",
    "            bet = 50\n",
    "        elif (row.Fighter_1_Odds <= -200):\n",
    "            bet = 25\n",
    "        else:\n",
    "            bet = 0\n",
    "    if row.Prediction_rf == 0:\n",
    "        if (row.Fighter_2_Odds > 100) & (row.Fighter_2_Odds < 150):\n",
    "            bet = 100\n",
    "        elif (row.Fighter_2_Odds >= 150) & (row.Fighter_2_Odds < 200):\n",
    "            bet = 250\n",
    "        elif (row.Fighter_2_Odds >= 200):\n",
    "            bet = 500\n",
    "        elif (row.Fighter_2_Odds <= 100) & (row.Fighter_2_Odds > -150):\n",
    "            bet = 75\n",
    "        elif (row.Fighter_2_Odds <= -150) & (row.Fighter_2_Odds > -200):\n",
    "            bet = 50\n",
    "        elif (row.Fighter_2_Odds <= -200):\n",
    "            bet = 25\n",
    "        else:\n",
    "            bet = 0\n",
    "    return bet\n",
    "\n",
    "def calculate_payoff_and_result(row):\n",
    "    # Calculating Payoff\n",
    "    if row.Prediction_rf == 1:\n",
    "        if row.Fighter_1_Odds>0:\n",
    "            payoff = (row.Fighter_1_Odds/100)*row.Bet\n",
    "        else:\n",
    "            payoff = row.Bet/((abs(row.Fighter_1_Odds)/100))\n",
    "    else:\n",
    "        if row.Fighter_2_Odds>0:\n",
    "            payoff = (row.Fighter_2_Odds/100)*row.Bet\n",
    "        else:\n",
    "            payoff = row.Bet/((abs(row.Fighter_2_Odds)/100))\n",
    "    # Calculating Bet Result\n",
    "    if row.Prediction_rf == row.result:\n",
    "        bet_result = payoff\n",
    "    else:\n",
    "        bet_result = -(row.Bet)\n",
    "    \n",
    "    return bet_result\n",
    "            \n",
    "data_rf = data[['fighter_1', 'fighter_2', 'Fighter_1_Odds', 'Fighter_2_Odds', 'Prediction_rf', 'result']].reset_index(drop = True)\n",
    "data_rf['Bet'] = data_rf.apply(rf_bets, axis = 1)\n",
    "data_rf['Bet_Result'] = data_rf.apply(calculate_payoff_and_result, axis = 1)\n",
    "\n",
    "data_rf.Bet_Result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fighter_1</th>\n",
       "      <th>fighter_2</th>\n",
       "      <th>Fighter_1_Odds</th>\n",
       "      <th>Fighter_2_Odds</th>\n",
       "      <th>Prediction_rf</th>\n",
       "      <th>result</th>\n",
       "      <th>Bet</th>\n",
       "      <th>Bet_Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>SergheiSpivac</td>\n",
       "      <td>GregHardy</td>\n",
       "      <td>-205.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>12.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>JalinTurner</td>\n",
       "      <td>JamieMullarkey</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>31.055901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>BrianKelleher</td>\n",
       "      <td>UmarNurmagomedov</td>\n",
       "      <td>560.0</td>\n",
       "      <td>-1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>DevonteSmith</td>\n",
       "      <td>LudovitKlein</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>31.055901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>DustinJacoby</td>\n",
       "      <td>MichalOleksiejczuk</td>\n",
       "      <td>-222.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>11.261261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>ThiagoSantos</td>\n",
       "      <td>MagomedAnkalaev</td>\n",
       "      <td>400.0</td>\n",
       "      <td>-800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>MarlonMoraes</td>\n",
       "      <td>SongYadong</td>\n",
       "      <td>220.0</td>\n",
       "      <td>-310.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>8.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>SodiqYusuff</td>\n",
       "      <td>AlexCaceres</td>\n",
       "      <td>-303.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>8.250825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>DrewDober</td>\n",
       "      <td>TerranceMcKinney</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>29.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>AlexPereira</td>\n",
       "      <td>BrunoSilva</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>MatthewSemelsberger</td>\n",
       "      <td>AJFletcher</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>TrevinJones</td>\n",
       "      <td>JavidBasharat</td>\n",
       "      <td>122.0</td>\n",
       "      <td>-190.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>26.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>DamonJackson</td>\n",
       "      <td>KamuelaKirk</td>\n",
       "      <td>-140.0</td>\n",
       "      <td>-111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>53.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>DalchaLungiambula</td>\n",
       "      <td>CodyBrundage</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>KrisMoutinho</td>\n",
       "      <td>GuidoCannetti</td>\n",
       "      <td>-184.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>TafonNchukwi</td>\n",
       "      <td>AzamatMurzakanov</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-227.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>11.013216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>AlexanderVolkov</td>\n",
       "      <td>TomAspinall</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>32.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ArnoldAllen</td>\n",
       "      <td>DanHooker</td>\n",
       "      <td>-121.0</td>\n",
       "      <td>-115.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>61.983471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>PaddyPimblett</td>\n",
       "      <td>KazulaVargas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>GunnarNelson</td>\n",
       "      <td>TakashiSato</td>\n",
       "      <td>-800.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>JaiHerbert</td>\n",
       "      <td>IliaTopuria</td>\n",
       "      <td>400.0</td>\n",
       "      <td>-650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>3.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>MikeGrundy</td>\n",
       "      <td>MakwanAmirkhani</td>\n",
       "      <td>-305.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>ShamilAbdurakhimov</td>\n",
       "      <td>SergeiPavlovich</td>\n",
       "      <td>250.0</td>\n",
       "      <td>-400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>6.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>NikitaKrylov</td>\n",
       "      <td>PaulCraig</td>\n",
       "      <td>-210.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>JackShore</td>\n",
       "      <td>TimurValiev</td>\n",
       "      <td>-140.0</td>\n",
       "      <td>-111.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>53.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>MuhammadMokaev</td>\n",
       "      <td>CodyDurden</td>\n",
       "      <td>-600.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>CurtisBlaydes</td>\n",
       "      <td>ChrisDaukaus</td>\n",
       "      <td>-600.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>4.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>MattBrown</td>\n",
       "      <td>BryanBarberena</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>-75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>AskarAskarov</td>\n",
       "      <td>KaiKara-France</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>MarcDiakiese</td>\n",
       "      <td>ViacheslavBorshchev</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-158.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>115.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>NeilMagny</td>\n",
       "      <td>MaxGriffin</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>ChrisGutierrez</td>\n",
       "      <td>BatgerelDanaa</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>AliaskhabKhizriev</td>\n",
       "      <td>DenisTiuliulin</td>\n",
       "      <td>-1400.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>MatheusNicolau</td>\n",
       "      <td>DavidDvorak</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>LuisSaldana</td>\n",
       "      <td>BrunoSouza</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-159.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>VicenteLuque</td>\n",
       "      <td>BelalMuhammad</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>-25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>CaioBorralho</td>\n",
       "      <td>GadzhiOmargadzhiev</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-139.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>71.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>MiguelBaeza</td>\n",
       "      <td>AndreFialho</td>\n",
       "      <td>-189.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>-50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>PatSabatini</td>\n",
       "      <td>TJLaramie</td>\n",
       "      <td>-750.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>MounirLazzez</td>\n",
       "      <td>AngeLoosa</td>\n",
       "      <td>-5000.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>DevinClark</td>\n",
       "      <td>WilliamKnight</td>\n",
       "      <td>-190.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>26.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>DrakkarKlose</td>\n",
       "      <td>BrandonJenkins</td>\n",
       "      <td>-800.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>RafaGarcia</td>\n",
       "      <td>JesseRonson</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>ChrisBarnett</td>\n",
       "      <td>MartinBuday</td>\n",
       "      <td>240.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>JordanLeavitt</td>\n",
       "      <td>TreyOgden</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-148.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>ClayGuida</td>\n",
       "      <td>ClaudioPuelles</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>LandoVannata</td>\n",
       "      <td>CharlesJourdain</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>-139.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>53.956835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>JordanWright</td>\n",
       "      <td>Marc-AndreBarriault</td>\n",
       "      <td>150.0</td>\n",
       "      <td>-220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>11.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>DwightGrant</td>\n",
       "      <td>SergeyKhandozhko</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>55.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>TysonPedro</td>\n",
       "      <td>IkeVillanueva</td>\n",
       "      <td>-900.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fighter_1            fighter_2  Fighter_1_Odds  Fighter_2_Odds  \\\n",
       "200        SergheiSpivac            GregHardy          -205.0           142.0   \n",
       "201          JalinTurner       JamieMullarkey          -161.0           120.0   \n",
       "202        BrianKelleher     UmarNurmagomedov           560.0         -1500.0   \n",
       "203         DevonteSmith         LudovitKlein          -161.0           116.0   \n",
       "204         DustinJacoby   MichalOleksiejczuk          -222.0           152.0   \n",
       "205         ThiagoSantos      MagomedAnkalaev           400.0          -800.0   \n",
       "206         MarlonMoraes           SongYadong           220.0          -310.0   \n",
       "207          SodiqYusuff          AlexCaceres          -303.0           195.0   \n",
       "208            DrewDober     TerranceMcKinney          -170.0           120.0   \n",
       "209          AlexPereira           BrunoSilva          -225.0           155.0   \n",
       "210  MatthewSemelsberger           AJFletcher          -200.0           145.0   \n",
       "211          TrevinJones        JavidBasharat           122.0          -190.0   \n",
       "212         DamonJackson          KamuelaKirk          -140.0          -111.0   \n",
       "213    DalchaLungiambula         CodyBrundage          -150.0           100.0   \n",
       "214         KrisMoutinho        GuidoCannetti          -184.0           120.0   \n",
       "215         TafonNchukwi     AzamatMurzakanov           145.0          -227.0   \n",
       "216      AlexanderVolkov          TomAspinall           100.0          -152.0   \n",
       "217          ArnoldAllen            DanHooker          -121.0          -115.0   \n",
       "218        PaddyPimblett         KazulaVargas             NaN             NaN   \n",
       "219         GunnarNelson          TakashiSato          -800.0           360.0   \n",
       "220           JaiHerbert          IliaTopuria           400.0          -650.0   \n",
       "221           MikeGrundy      MakwanAmirkhani          -305.0           170.0   \n",
       "222   ShamilAbdurakhimov      SergeiPavlovich           250.0          -400.0   \n",
       "223         NikitaKrylov            PaulCraig          -210.0           155.0   \n",
       "224            JackShore          TimurValiev          -140.0          -111.0   \n",
       "225       MuhammadMokaev           CodyDurden          -600.0           350.0   \n",
       "226        CurtisBlaydes         ChrisDaukaus          -600.0           333.0   \n",
       "227            MattBrown       BryanBarberena          -110.0          -125.0   \n",
       "228         AskarAskarov       KaiKara-France          -455.0           290.0   \n",
       "229         MarcDiakiese  ViacheslavBorshchev           115.0          -158.0   \n",
       "230            NeilMagny           MaxGriffin          -300.0           210.0   \n",
       "231       ChrisGutierrez        BatgerelDanaa           110.0          -155.0   \n",
       "232    AliaskhabKhizriev       DenisTiuliulin         -1400.0           575.0   \n",
       "233       MatheusNicolau          DavidDvorak           100.0          -140.0   \n",
       "234          LuisSaldana           BrunoSouza           110.0          -159.0   \n",
       "235         VicenteLuque        BelalMuhammad          -200.0           140.0   \n",
       "236         CaioBorralho   GadzhiOmargadzhiev          -105.0          -139.0   \n",
       "237          MiguelBaeza          AndreFialho          -189.0           135.0   \n",
       "238          PatSabatini            TJLaramie          -750.0           335.0   \n",
       "239         MounirLazzez            AngeLoosa         -5000.0           155.0   \n",
       "240           DevinClark        WilliamKnight          -190.0           137.0   \n",
       "241         DrakkarKlose       BrandonJenkins          -800.0           400.0   \n",
       "242           RafaGarcia          JesseRonson           100.0          -160.0   \n",
       "243         ChrisBarnett          MartinBuday           240.0          -300.0   \n",
       "244        JordanLeavitt            TreyOgden           110.0          -148.0   \n",
       "245            ClayGuida       ClaudioPuelles          -113.0          -125.0   \n",
       "246         LandoVannata      CharlesJourdain          -110.0          -139.0   \n",
       "247         JordanWright  Marc-AndreBarriault           150.0          -220.0   \n",
       "248          DwightGrant     SergeyKhandozhko          -105.0          -135.0   \n",
       "249           TysonPedro        IkeVillanueva          -900.0           400.0   \n",
       "\n",
       "     Prediction_rf  result  Bet  Bet_Result  \n",
       "200              1       1   25   12.195122  \n",
       "201              1       1   50   31.055901  \n",
       "202              0       0   25    1.666667  \n",
       "203              1       1   50   31.055901  \n",
       "204              1       1   25   11.261261  \n",
       "205              0       0   25    3.125000  \n",
       "206              0       0   25    8.064516  \n",
       "207              1       1   25    8.250825  \n",
       "208              1       1   50   29.411765  \n",
       "209              1       1   25   11.111111  \n",
       "210              1       1   25   12.500000  \n",
       "211              0       0   50   26.315789  \n",
       "212              1       1   75   53.571429  \n",
       "213              1       0   50  -50.000000  \n",
       "214              1       0   50  -50.000000  \n",
       "215              0       0   25   11.013216  \n",
       "216              0       0   50   32.894737  \n",
       "217              1       1   75   61.983471  \n",
       "218              1       1    0         NaN  \n",
       "219              1       1   25    3.125000  \n",
       "220              0       0   25    3.846154  \n",
       "221              1       0   25  -25.000000  \n",
       "222              0       0   25    6.250000  \n",
       "223              1       0   25  -25.000000  \n",
       "224              1       1   75   53.571429  \n",
       "225              1       0   25  -25.000000  \n",
       "226              1       1   25    4.166667  \n",
       "227              1       0   75  -75.000000  \n",
       "228              1       0   25  -25.000000  \n",
       "229              1       1  100  115.000000  \n",
       "230              1       1   25    8.333333  \n",
       "231              1       1  100  110.000000  \n",
       "232              1       1   25    1.785714  \n",
       "233              1       1   75   75.000000  \n",
       "234              1       1  100  110.000000  \n",
       "235              1       0   25  -25.000000  \n",
       "236              1       1   75   71.428571  \n",
       "237              1       0   50  -50.000000  \n",
       "238              1       1   25    3.333333  \n",
       "239              1       1   25    0.500000  \n",
       "240              1       1   50   26.315789  \n",
       "241              1       1   25    3.125000  \n",
       "242              1       1   75   75.000000  \n",
       "243              0       0   25    8.333333  \n",
       "244              1       1  100  110.000000  \n",
       "245              0       0   75   60.000000  \n",
       "246              0       0   75   53.956835  \n",
       "247              0       0   25   11.363636  \n",
       "248              0       0   75   55.555556  \n",
       "249              1       1   25    2.777778  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rf[200:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fighter_1', 'weight_1', 'reach_1', 'age_1', 'slpm_1', 'sapm_1',\n",
       "       'td_avg_1', 'sub_avg_1', 'strk_acc_1', 'strk_def_1', 'td_acc_1',\n",
       "       'td_def_1', 'wins_1', 'losses_1', 'fighter_2', 'weight_2', 'reach_2',\n",
       "       'age_2', 'slpm_2', 'sapm_2', 'td_avg_2', 'sub_avg_2', 'strk_acc_2',\n",
       "       'strk_def_2', 'td_acc_2', 'td_def_2', 'wins_2', 'losses_2', 'result',\n",
       "       'SUB_OVR', 'KO_OVR', 'Fighter_1_Odds', 'Fighter_2_Odds', 'reach_diff',\n",
       "       'age_diff', 'slpm_diff', 'sapm_diff', 'td_acc_diff', 'td_def_diff',\n",
       "       'td_avg_diff', 'sub_avg_diff', 'strk_acc_diff', 'strk_def_diff',\n",
       "       'wins_diff', 'losses_diff', 'win_pct_1', 'win_pct_2', 'win_pct_diff',\n",
       "       'Prediction_1_lr', 'Prediction_2_lr', 'Prediction_rf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
